## å¯¼å…¥å„ç§åº“
import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from torchvision import datasets
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

# ä¸‹è½½æ•°æ®å¹¶å°†æ‰€æœ‰æ•°æ®è½¬æ¢ä¸º ğ“ğğ§ğ¬ğ¨ğ« ï¼Œtrainä¸ºæ˜¯å¦è®­ç»ƒæ•°æ®é›†ï¼Œdownloadé»˜è®¤ç½‘ä¸Šä¸‹è½½
data_path = r'E:\STUDYCONTENT\Pycharm\DeepLearningExp'
mnist_train = torchvision.datasets.FashionMNIST(data_path, train=True, download=False, transform=transforms.ToTensor())
mnist_test = torchvision.datasets.FashionMNIST(data_path, train=False, download=False, transform=transforms.ToTensor())

# é€šè¿‡ ğƒğšğ­ğšğ‹ğ¨ğšğğğ« è¯»å–å°æ‰¹é‡æ•°æ®æ ·æœ¬ï¼Œshuffleæ˜¯å¦æ‰“ä¹±é¡ºåºï¼Œnum_workersä¸ºè¿›ç¨‹çš„ä¸ªæ•°
batch_size = 128
train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)
test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=0)
len(train_iter)

## å‚æ•°åˆå§‹åŒ–
num_inputs = 784  # æ‹‰æˆå‘é‡çš„é•¿åº¦
num_hiddens = 256  # éšè—å±‚çš„ç¥ç»å…ƒæ•°
num_outputs = 10  # åˆ†ç±»çš„ç±»åˆ«ä¸ªæ•°
W1 = torch.normal(0, 0.01, size=(num_inputs, num_hiddens))  # æƒé‡çš„é«˜æ–¯éšæœºåˆå§‹åŒ–
b1 = torch.zeros(num_hiddens)  # åç§»çš„éšæœºåˆå§‹åŒ–
W2 = torch.normal(0, 0.01, size=(num_hiddens, num_outputs))  # æƒé‡çš„é«˜æ–¯éšæœºåˆå§‹åŒ–
b2 = torch.zeros(num_outputs)
params = [W1, b1, W2, b2]

for param in params:
    param.requires_grad_(requires_grad=True)


# é€‰ç”¨å¸¸ç”¨ReLUä½œä¸ºæ¿€æ´»å‡½æ•°
# def relu(z):
#   z=np.where(z>0,z,0)
#   return  torch.tensor(z)
def relu(X):  # æ¿€æ´»å‡½æ•°
    return torch.max(input=X, other=torch.tensor(0.0))
    # ä½¿ç”¨åŸºç¡€çš„maxå‡½æ•°æ¥å®ç°ReLUï¼Œè€Œéç›´æ¥è°ƒç”¨reluå‡½æ•°


def tanh(X):
    return torch.tanh(X)


def sigmoid(X):
    return torch.sigmoid(X)


## ç½‘ç»œ
# å®šä¹‰Softmax
def softmax(X):
    return torch.exp(X) / torch.exp(X).sum()  # è¿™é‡Œåº”ç”¨äº†å¹¿æ’­æœºåˆ¶


# å®šä¹‰ç½‘ç»œæ¨¡å‹

def net(X):  # å®šä¹‰æ¨¡å‹ç»“æ„
    X = X.view((-1, num_inputs))  # viewå‡½æ•°å°†æ¯å¼ åŸå§‹å›¾åƒæ”¹æˆé•¿åº¦ä¸ºnum_inputsçš„å‘é‡
    H = sigmoid(torch.matmul(X, W1) + b1)
    return torch.matmul(H, W2) + b2


#    return nn.LogSoftmax(torch.matmul(H, W2) + b2)
## äº¤å‰ç†µæŸå¤±
loss = torch.nn.CrossEntropyLoss()  # æŸå¤±å‡½æ•°


## äº¤å‰ç†µæŸå¤±
# loss=nn.NLLloss()

## æ¢¯åº¦ä¸‹é™
def sgd(params, lr, batch_size):
    for param in params:
        param.data -= lr * param.grad / batch_size  # æ³¨æ„è¿™é‡Œæ›´æ”¹paramæ—¶ç”¨çš„param.data


# è®¡ç®—æ¨¡å‹åœ¨æŸä¸ªæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡
def evaluate_accuracy(data_iter, net):
    acc_sum, n = 0.0, 0
    for X, y in data_iter:
        acc_sum += (softmax(net(X)).argmax(dim=1) == y).float().sum().item()
        n += y.shape[0]
    return acc_sum / n


# å®šä¹‰æ¨¡å‹è®­ç»ƒå‡½æ•°
def train(net, train_iter, test_iter, num_epochs, batch_size,
          params, optimizer=None):
    train_set, test_set = [], []
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
        for X, y_hat in train_iter:
            y = net(X)
            yt = softmax(y)
            l = loss(y, y_hat).sum()

            # æ¢¯åº¦æ¸…é›¶
            if optimizer is not None:
                optimizer.zero_grad()  # è¿™é‡Œæˆ‘ä»¬ç”¨åˆ°ä¼˜åŒ–å™¨ï¼Œæ‰€ä»¥ç›´æ¥å¯¹ä¼˜åŒ–å™¨è¡Œæ¢¯åº¦æ¸…é›¶
            elif params is not None and params[0].grad is not None:
                for param in params:
                    param.grad.data.zero_()

            l.backward()
            if optimizer is None:
                sgd(params, lr, batch_size)
            else:
                optimizer.step()  # ç”¨åˆ°ä¼˜åŒ–å™¨è¿™é‡Œ

            train_l_sum += l.item()
            train_acc_sum += (yt.argmax(dim=1) == y_hat).sum().item()
            n += y_hat.shape[0]
        test_acc = evaluate_accuracy(test_iter, net)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'
              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))

        # è®°å½•loss
        train_item, test_item = float(train_l_sum / n), float(test_acc)
        train_set.append(train_item)
        test_set.append(test_item)

    return train_set, test_set


# æ¨¡å‹lossç”»å›¾
def plot_loss(train_loss, val_loss):
    epochs = len(train_loss)
    x = range(epochs)
    plt.plot(x, train_loss, label='Train')
    plt.plot(x, val_loss, label='Validation')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()


## æ¨¡å‹è®­ç»ƒ
lr = 0.1
num_epochs = 20  # è¿­ä»£æ¬¡æ•°
trains, tests = train(net, train_iter, test_iter, num_epochs, batch_size, params)

plot_loss(trains, tests)
